---
title: "Blog2"
author: "Michael Ippolito"
date: '2022-10-22'
output:
  html_document: default
  pdf_document: default
---

## Executive summary

Part of UW-Madison's "Cybersecurity to the Edge" (C2E) initiative requires estimating the number of UW-Madison-owned endpoints that are running the Qualys Cloud Agent and/or Cisco Secure Endpoint (formerly Cisco AMP), along with the progress of each division in attaining full deployment of the software on all of its endpoints. While the endpoint counts running the software are easily obtained, the total number of endpoints in each division isn't readily available To address this gap, linear models were constructed to leverage known data points in an attempt to estimate total endpoint counts per division. First, mac address counts were gathered from the Authorized Agent Network Tool Suite (AANTS) database, which provides a current count of network-connected devices per IP subnet. This data is of limited quality, as it is difficult to correlate IP subnet to UW division. Second, FTE counts were collected from UW's Tableau instance. Next, actual endpoint counts were gathered from departments having full or near-full deployments of Qualys and AMP. Various linear models were fitted to estimate a quantitative relationship between endpoint counts and mac address and/or FTE counts. To account for units having an atypically large number of endpoints (for example, central service providers), additional categorical variables were manually applied to the data using institutional knowledge. An additional categorical variable was applied to DoIT departments to account for the comparatively high number of endpoints relative to other campus units.

The best model exhibited a strong statistical correlation between endpoint count and FTE count (including adjustments for large and DoIT units), which yielded an adjusted R-squared value of 0.881 (see model analysis below for a complete discussion of the significance of this value). Commonly used techniques to evaluate the model's residuals (how far the data points deviate from the best-fit line) indicate that the model is statistically valid. The total predicted endpoint point estimate of 86,130; however, the 90% prediction interval includes a wide range of values, resulting in lower and upper bounds of 43,600 and 169,000, respectively. Based on current deployment totals, we estimate the campus to have deployed either Qualys or AMP on approximately 40,900 computers (47.5%).

It should be noted that the counts generated by the model are rough esimates only and should not be used as a checklist against which a particular unit's progress can be measured. It is further noted that these counts only estimate the number of endpoints upon which Qualys and Amp can be installed; it excludes devices such as printers, IOT devices, and devices running operating systems that are incompatible with Qualys and Amp.


## Objective

This analysis was performed as a requirement for UW-Madison's C2E initiative, which aims to assess the extent to which the university has deployed Cisco Secure Endpoint and Qualys Cloud Agent software on its endpoints. Because departments or divisions maintain their own device inventory (or lack a device inventory altogether), it has been historically difficult for DoIT and the Office of Cybersecurity to assess how many endpoints each division has.

The goal of this analysis is to generate rough endpoint estimates for each division. Our approach is to gather data from departments and divisions from which we have known endpoint counts, fit a linear model to that data, and predict endpoint counts based on the coefficients generated by the model.

The data we have with which to create this kind of model is of limited quality. First, we have FTE counts per department and division (we used March 2022 payroll data). Second, we have mac address counts mapped to IP addresses over a 30-day period during July and August 2022. While the quality of the FTE counts itself is high-fidelity, endpoint counts and FTE counts don't correlate precisely. Likewise, the fidelity of the mac-address data suffers from several problems. First, it is difficult to correlate IP addresses (and, hence, mac addresses) back to a UDDS code. I created a separate model to do this, but the model is only 81% accurate against validated training data and is likely much less accurate against untested data. Second, the mac address data doesn't account for endpoints that are connected to public networks like UWNet or Eduroam, nor does it account for VPN-connected or other remote users.

In addition to FTE and mac address data, a categorical variable (large_unit) was added to account for divisions and departments having atypically high endpoint counts, for example units that host services for other campus units that may require large numbers of servers. We also treated DoIT as a special unit, since it runs many services for campus and will, therefore, have an unusually high endpoint count.

Despite the limitations described above, the model may serve as a rough estimate in the absence of any other means of quantifying endpoint counts. These numbers should only be used with a heavy disclaimer and should not be used as a checklist against which to measure the true progress of any school, college, or division in deploying security software to their endpoints.


## Data sources

### Mac address count data

For the first data set, we queried mac address counts from the AANTS database for the last 30 days. Example data:

ipstart-ipend:count \
10.150.14.0-10.150.15.255:228

There were approximately 32,000 subnet records like the above. These counts had to be correlated back to a UDDS code; historically this has always been a challenge for the UW, since AANTS does not tie a subnet record to UDDS. However, each subnet *is* tied back to the staff members who administer it. And from these staff members we can query UW's WhoIs database to glean the UDDS code for each of those staff members. This isn't always reliable, as the staff member isn't always a member of the UDDS the subnet belongs to. For example, SMPH network and security staff are listed as administrative and technical contacts for almost every SMPH subnet; but those staff members have a UDDS code that corresponds to SMPH Administration, rather than the actual SMPH department. Likewise, DoIT Departmental Support (DS) administers many departments' endpoints on behalf of those departments, erroneoulsy resulting in a UDDS code of A06.


### Subnet-to-UDDS correlation

To overcome these challenges, I combined the contact-based UDDS predictions with a natural language processing (NLP) model based on document similarity. The subnet names and descriptions were quantitatively compared with terms scraped from websites mapped back to a particular UDDS. To calculate the "best" UDDS match, the UDDS with the smallest Jaccard distance was weighted more heavily than those with larger distances. Additional "institutional knowledge" was also used to weight some UDDS predictions more heavily than others. The following is an example subnet record; the "udds" field is an array of UDDS predictions based on probability values, with the UDDS having the highest probability listed first:

ipstart,ipend,ipstarthi,ipendhi,udds,alloc,netname,descr,match,train,correct,tie,ipastart,ipaend,predicted_div,predicted_dept \
2421981696,2421982207,0,0,[{'udds': 'A48 - L&S', 'dept': 'ATMOSPHERIC & OCEANIC SCIENCES', 'p': 0.75, 'ct': 1.5}, {'udds': 'A40 - Nelson Inst', 'dept': 'CENTER FOR CLIMATIC RESEARCH', 'p': 0.25, 'ct': 1.0}],ASSIGNMENT,AOS-LAN,AOS-Meteorology,0,0,0,144.92.130.0,144.92.131.255,A48 - L&S,ATMOSPHERIC & OCEANIC SCIENCES


### FTE counts

With the aim of correlating server counts directly to FTE counts, we collected the March 2022 FTE counts from UW's Tableau instance. Example data:

Tableau_id,UDDS,fte_count
589,A349100,177.5


## Load the data

We'll first need to load the appropriate R libraries.

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(car)
library(faraway)
library(lmtest)

```


### Load mac address data

To begin with, the strategy is to combine Wiscnic subnet data with Wisnic mac address data, so that we get a count of mac addresses by subnet. These are fabricated examples of the two sources we have for this:

WiscNic subnet data:

| Subnet | Predicted Divcode |
| ------ | ----------------- |
| 10.0.0.0/24 | A01 |
| 10.1.0.0/24 | A02 |
| 10.2.0.0/24 | A03 |

WiscNic mac address data:

| Mac Address | IP Address | Timestamp |
| ----------- | ---------- | --------- |
| 001122334455 | 10.0.0.1 | 2022-08-01 00:00:00 |
| 002233445566 | 10.1.0.1 | 2022-08-02 00:00:00 |
| 003344556677 | 10.2.0.1 | 2022-08-03 00:00:00 |

We'll merge and group them together to get something like this:

| Divcode | Mac Address Count |
| ------- | ----------------- |
| A01 | 3 |
| A02 | 50 |
| A03 | 96 |

```{r}

# Data was collected and parsed using /home/ippolito/c2e.py

# Load wiscnic data, including predicted div/dept codes
dfwiscnic_raw <- read.csv("/Users/ippolito/Documents/wiscnic2udds.csv")
dfwisnic <- dfwiscnic_raw %>%
    arrange(ipstarthi, ipstart, ipendhi, ipend)

# Load physaddr data - ipv4 - for 60 days prior to 8/9/2022
dfmac4_raw <- read.csv("/Users/ippolito/Documents/physaddr_2.csv")
dfmac4 <- dfmac4_raw

# Load physaddr data - ipv6 - for 60 days prior to 8/9/2022 - not used yet
#dfmac6_raw <- read.csv("/Users/ippolito/Documents/physaddr6_2.csv")
#dfmac6 <- dfmac6_raw

# Group by predicted div code
dfmac4_grp <- dfmac4 %>%
    group_by(pred_divcode) %>%
    summarize(mac_ct=n())

```


### Load FTE data

Next, load FTE data which we collected by scraping our Tableau instance.

```{r}

# FTE counts - division-level counts only - 2021
#dffte_raw <- read.csv("/Users/ippolito/Documents/division-fte-counts.csv")
#dffte <- dffte_raw %>%
#    mutate(fte_ct = sub(',', '', FTE)) %>%
#    mutate(Total.Headcount = sub(',', '', Total.Headcount)) %>%
#    mutate(fte_ct = as.numeric(fte_ct), Total.Headcount = as.numeric(Total.Headcount)) %>%
#    rename(divcode=UDDS) %>%
#    arrange(divcode)

# FTE counts - dept-level
dffte_raw <- read.csv("/Users/ippolito/Documents/tableau-parse-2022-03.csv")
dffte <- dffte_raw %>%
    mutate(div = substr(udds, 1, 3)) %>%
    mutate(dept = substr(udds, 1, 5))

# Group by div code
dffte_grp <- dffte %>%
    group_by(div) %>%
    summarize(fte_ct=sum(fte_count)) %>%
    rename(divcode=div) %>%
    arrange(divcode)

```


### Load Qualys and AMP data

Next, load the Qualys and Cisco Secure Endpoint (AKA Cisco AMP) endpoint counts, which we exported from Elastic.

```{r}

# Load Qualys computer data
dfqualys_raw <- read.csv("/Users/ippolito/Documents/qualys_computers2.csv")
dfqualys <- dfqualys_raw %>% 
    rename(udds = qualys.agentInfo.activationKey.title, netname = source.geo.wiscnic.netname) %>%
    arrange(udds) %>%
    filter(substr(udds, 1, 1) == 'A') %>%
    mutate(divcode = substr(udds, 1, 3)) %>%
    mutate(deptcode = substr(udds, 1, 5))

# Some deptcodes look like this: A45-L (truncated from A45-Law-School-WithAV) - deal with these
dfqualys <- dfqualys %>%
    mutate(deptcode = ifelse(grepl('^A\\d\\d-[A-Za-z]$', deptcode), paste0(substr(deptcode, 1, 3), '00'), deptcode))

# Group by div code
dfqualys_grp <- dfqualys %>%
    group_by(divcode) %>%
    summarize(ct=n()) %>%
    arrange(divcode) %>%
    rename(qualys_ct=ct)

# Group by dept code
dfqualys_grp2 <- dfqualys %>%
    group_by(divcode, deptcode) %>%
    summarize(ct=n(), .groups='keep') %>%
    arrange(divcode, deptcode) %>%
    rename(qualys_ct=ct)

# Load AMP computer data
dfamp_raw <- read.csv("/Users/ippolito/Documents/amp_computers.csv")
dfamp <- dfamp_raw %>% 
    rename(udds = cisco.amp.computer.group_name, netname = source.geo.wiscnic.netname) %>%
    mutate(udds = ifelse(substr(udds, 1, 3) == 'DS-', 'A0600', udds)) %>%    # deal with SA and PCI
    arrange(udds) %>%
    filter(substr(udds, 1, 1) == 'A') %>%
    mutate(divcode = substr(udds, 1, 3)) %>%
    mutate(deptcode = substr(udds, 1, 5)) %>%
    filter(divcode != 'A00')

# Some deptcodes look like this: A45-L (truncated from A45-Law-School-WithAV) - deal with these
dfamp <- dfamp %>%
    mutate(deptcode = ifelse(grepl('^A\\d\\d-[A-Za-z]$', deptcode), paste0(substr(deptcode, 1, 3), '00'), deptcode))

# Group by div code
dfamp_grp <- dfamp %>%
    group_by(divcode) %>%
    summarize(ct=n()) %>%
    filter(divcode != 'Aud') %>%
    arrange(divcode) %>%
    rename(amp_ct=ct)

# Group by dept code
dfamp_grp2 <- dfamp %>%
    group_by(divcode, deptcode) %>%
    summarize(ct=n(), .groups='keep') %>%
    filter(divcode != 'Aud') %>%
    arrange(divcode, deptcode) %>%
    rename(amp_ct=ct)

```


### Units with the most complete data

In order to estimate device counts for units that have incomplete or no data, we'll gather a list of units that *do* have full or relatively full deployments of either AMP or Qualys. Then we'll try to correlate device counts from those departments to mac address and FTE counts to see if a statistically significant relationship exists. Based on the results, we'll try to fit a linear model to the rest of the units to estimate device counts.

```{r}

# Units with the most complete data
best_divs <- c(
    'A06',  # doit
    'A54',  # nursing
    'A45',  # law
    'A56'   # pharmacy
)
best_depts <- c(
    #'A0670',  # doit seo
    'A0746',  # nutritional sci
    'A0780',  # ag research sta
    'A0713',  # dairy research ctr
    'A0703',  # admin svcs
    'A0764',  # forest & wildlife ecol
    'A0720',  # ag & applied econ
    'A0775',  # admin svcs ctr/russell lab
    'A0715',  # ctr for integrated ag syst
    'A0778',  # wi crop innovation ctr
    'A0742',  # genetics
    'A0761',  # enivron resource ctr
    'A0736',  # entomology
    'A0748',  # plant path
    'A0752',  # comm & environ soc
    'A0754',  # soil sci
    'A0728',  # bacteriology
    'A0730',  # biochem
    'A0725',  # animal & dairy sci
    'A0724',  # life sci comm
    'A4881',  # social science research svc
    'A4854',  # math
    'A4851',  # library & info studies
    'A4857',  # atmospheric science
    'A4820',  # comp sci
    'A4894',  # urban & regional planning
    'A4888',  # survey ctr
    'A4844',  # lafollete public affairs
    'A4850',  # learning supp
    'A4838',  # history
    'A4818',  # comm sci & disorders
    'A5398',  # urology
    'A5361',  # ortho
    'A5333',  # human oncology
    'A5360',  # ophth
    'A5397',  # surg
    'A5342',  # dom
    'A5312',  # biostat
    'A5328',  # ob/gyn
    'A5385',  # pop health
    'A5302',  # smph admin
    'A5351',  # neurology
    'A5348',  # med phys
    'A5367'   # peds
)

# Define "large" customers, e.g. central service providers or customers providing quantitative computing services
large_divs <- c(
    #'A06',    # doit
    'A19',    # engr
    'A46',    # wpm
    'A52',    # slh
    'A85'     # uw housing
)
large_depts <- c(
    'A0208',  # aims
    #'A0670',  # doit
    'A3491',  # space sci & engr
    'A3493',  # ice cube
    'A4820',  # cs
    'A4851',  # library
    'A4881',  # social science research svc
    'A5302',  # smph admin
    #'A5312',  # biostat
    'A5342',  # dept of medicine
    'A5361'   # ortho
)

```


## Data wrangling

The data will need to be grouped by division code and aggregated. The linear model tries to model endpoint counts based on real counts, but the real counts available to use consist of separate Qualys and AMP counts. We'll need a single number to use for the "real" count. For this, we'll use either the Qualys count or the AMP count, whichever is higher. We'll call this single number the "endpoint count."

```{r}

# Group mac address counts by predicted div code
dfmac4_grp_divs <- dfmac4 %>%
    filter(pred_divcode %in% best_divs) %>%
    group_by(pred_divcode) %>%
    summarize(mac_ct=n()) %>%
    rename(unit=pred_divcode)

# Group mac address counts by predicted dept code
dfmac4_grp_depts <- dfmac4 %>%
    mutate(dept=ifelse(pred_deptcode=='A5398', 'A5361', pred_deptcode)) %>%    # urology is support by orthopedics
    filter(pred_deptcode %in% best_depts) %>%
    group_by(pred_deptcode) %>%
    summarize(mac_ct=n()) %>%
    rename(unit=pred_deptcode)

# Group qualys data by div code
dfqualys_grp_divs <- dfqualys %>%
    filter(divcode %in% best_divs) %>%
    group_by(divcode) %>%
    summarize(qualys_ct=n()) %>%
    rename(unit=divcode)

# Group qualys data by dept code
dfqualys_grp_depts <- dfqualys %>%
    filter(deptcode %in% best_depts) %>%
    group_by(deptcode) %>%
    summarize(qualys_ct=n()) %>%
    rename(unit=deptcode)

# Aggregate qualys counts
dfqualys_grp_units <- rbind(dfqualys_grp_divs, dfqualys_grp_depts)

# Group amp data by div code
dfamp_grp_divs <- dfamp %>%
    filter(divcode %in% best_divs) %>%
    group_by(divcode) %>%
    summarize(amp_ct=n()) %>%
    rename(unit=divcode)

# Group amp data by dept code
dfamp_grp_depts <- dfamp %>%
    filter(deptcode %in% best_depts) %>%
    group_by(deptcode) %>%
    summarize(amp_ct=n()) %>%
    rename(unit=deptcode)

# Aggregate amp counts
dfamp_grp_units <- rbind(dfamp_grp_divs, dfamp_grp_depts)

# Group fte data by div code
dffte_grp_divs <- dffte %>%
    filter(div %in% best_divs) %>%
    group_by(div) %>%
    summarize(fte_ct=sum(fte_count)) %>%
    rename(unit=div)

# Group fte data by dept code
dffte_grp_depts <- dffte %>%
    mutate(dept=ifelse(dept=='A5398', 'A5361', dept)) %>%    # urology is support by orthopedics
    filter(dept %in% best_depts) %>%
    group_by(dept) %>%
    summarize(fte_ct=sum(fte_count)) %>%
    rename(unit=dept)

# Aggregate fte counts
dffte_grp_units <- rbind(dffte_grp_divs, dffte_grp_depts)

# Aggregate unit data
dfagg <- rbind(dfmac4_grp_divs, dfmac4_grp_depts)
dfagg <- dfagg %>%
    full_join(dfqualys_grp_units, by=c('unit')) %>%
    full_join(dfamp_grp_units, by=c('unit')) %>%
    full_join(dffte_grp_units, by=c('unit')) %>%
    mutate(qualys_ct=replace_na(qualys_ct, 0)) %>%
    mutate(amp_ct=replace_na(amp_ct, 0))

# Calculate the highest between the amp and qualys count, which will be used for the predictions
dfagg$endpoint_ct = pmax(dfagg$qualys_ct, dfagg$amp_ct)

# Code for large units
large_units <- c(large_divs, large_depts)
dfagg <- dfagg %>%
    mutate(doit_unit=ifelse(unit == 'A06', TRUE, FALSE)) %>%
    mutate(large_unit=ifelse(unit %in% large_units, TRUE, FALSE)) %>%
    #mutate(large_unit=ifelse(unit == 'A06', 2, ifelse(unit %in% large_units, 1, 0))) %>%
    arrange(unit)
dfagg$doit_unit <- factor(dfagg$doit_unit)
dfagg$large_unit <- factor(dfagg$large_unit)

# Filter out points of high influence
dfagg <- dfagg %>%
    filter(unit != 'A5342')

# Display table
dfagg

# Redacted
dfagg_red <- dfagg %>%
    rename(central_it = doit_unit, vuln_mgmt_ct = qualys_ct, antimalware_ct = amp_ct) %>%
    mutate(unit = gsub('A[0-9][0-9]', 'Axx', unit))
dfagg_red

```


## Scatterplots

Visually compare the data.

```{r}

# Graph mac addresses vs endpoints
dfagg %>%
    ggplot(aes(x=mac_ct, y=endpoint_ct)) +
    geom_point()

# Graph FTEs vs endpoints (all units)
dfagg %>%
    ggplot(aes(x=fte_ct, y=endpoint_ct)) +
    geom_point() +
    ggtitle('FTE vs endpoints (all units)')

# Graph FTEs vs endpoints (no large units)
dfagg %>%
    #filter(large_unit != TRUE & doit_unit != TRUE) %>%
    filter(large_unit != TRUE) %>%
    ggplot(aes(x=fte_ct, y=endpoint_ct)) +
    geom_point() +
    ggtitle('FTE vs endpoint (no large units)')

# Graph FTEs vs endpoints (large units only)
dfagg %>%
    #filter(large_unit == TRUE | doit_unit == TRUE) %>%
    filter(large_unit == TRUE) %>%
    ggplot(aes(x=fte_ct, y=endpoint_ct)) +
    geom_point() +
    ggtitle('FTE vs endpoint (large units only)')

# Endpoint count vs large_unit
boxplot(endpoint_ct ~ large_unit, dfagg)

```


## Linear modeling

For a first pass, try fitting without doing any transformations. We'll try using various combinations of parameters.

```{r}

#dfagg <- dfagg %>%
#    filter(unit != 'A5342')

# List of formulas to use in models
lmod_form <- c(
    'endpoint_ct ~ mac_ct',
    'endpoint_ct ~ fte_ct',
    'endpoint_ct ~ fte_ct + mac_ct',
    'endpoint_ct ~ fte_ct + mac_ct + fte_ct:mac_ct',
    'endpoint_ct ~ mac_ct + large_unit',
    'endpoint_ct ~ fte_ct + large_unit',
    'endpoint_ct ~ large_unit + fte_ct + mac_ct',
    'endpoint_ct ~ large_unit + fte_ct + mac_ct + fte_ct:mac_ct',
    'endpoint_ct ~ fte_ct + large_unit + fte_ct:large_unit',
    'endpoint_ct ~ fte_ct:large_unit',
    'endpoint_ct ~ fte_ct + large_unit + doit_unit',
    'endpoint_ct ~ fte_ct + large_unit:fte_ct + doit_unit:fte_ct',
    'endpoint_ct ~ fte_ct + large_unit:fte_ct + doit_unit',
    'endpoint_ct ~ fte_ct + large_unit + doit_unit:fte_ct',
    'endpoint_ct ~ fte_ct + large_unit:doit_unit'
)

# Initialize list to store model results
lmod <- vector(mode='list', length=length(lmod_form))

# Iterate through models
for (i in seq(1, length(lmod_form))) {
    lmod[[i]] <- lm(lmod_form[i], na.action=na.omit, data=dfagg)
    print("----------------------------------------------------------------------------------------------")
    print(paste0(i, ': ', lmod_form[i]))
    print(summary(lmod[[i]]))
}

```


## Model analysis

While R-squared values provide evidence of correlation, other criteria must be met before assessing whether a linear model is valid. First, an analysis of residuals must be performed. A residual is a measure of the difference between an observed value in the response variable (in this case, endpoint count) and the value predicted by the model. For the model to be considered valid, a histogram of the residuals should be fairly normal; i.e., there should be minimal skew, with the resulting curve looking like a bell curve. In addition, a graph of fitted values versus residuals should be fairly homoschedastic; i.e., there should be minimal variability between the fitted values and residuals.

First, we'll define a function to perform the residual analysis.

```{r}

# Define function to calculate mean squared error
mse <- function(lmod) {
  return(mean((summary(lmod))$residuals ^ 2))
}

# Define function to aid in model analysis
ModelAnalysis <- function(lmod, title) {

  # Plot residuals
  print('--------------------------------------------------')
  print(title)
  print(lmod$call)
  par(mfrow=c(2, 2))
  plot(lmod)
  mtext(title, side=3, outer=T, line=-1)

  # Shapiro test to determine normality of residuals
  # null hypothesis: the residuals are normal
  # p-value is small, so reject the null
  # i.e., the residuals are not normal
  st <- shapiro.test(lmod$residuals)
  stret <- 'not normal'
  if (st$p.value <= 0.05) {
    print(paste0("Shapiro test for normality: The p-value of ", st$p.value, " is <= 0.05, so reject the null; i.e., the residuals are NOT normal."))
  } else {
    print(paste0("Shapiro test for normality: The p-value of ", st$p.value, " is > 0.05, so do not reject the null; i.e., the residuals are normal."))
    stret <- 'normal'
  }
 
  # Breusch-Pagan test to determine homoschedasticity of residuals
  bp <- bptest(lmod)
  bpret <- 'inconclusive'
  if (bp$p.value > 0.05 & bp$statistic < 10) {
      print(paste0("Breusch-Pagan test for homoschedasticity: The p-value of ", bp$p.value, " is > 0.05 and the test statistic of ", bp$statistic,
          " is < 10, so don't reject the null; i.e., the residuals are homoschedastic."))
      bpret <- 'homoschedastic'
  } else if (bp$p.value <= 0.05) {
      print(paste0("Breusch-Pagan test for homoschedasticity: The p-value of ", bp$p.value, " is <= 0.05 and the test statistic is ", bp$statistic,
          ", so reject the null; i.e., the residuals are heteroschedastic."))
      bpret <- 'heteroschedastic'
  } else {
      print(paste0("Breusch-Pagan test for homoschedasticity: The p-value of ", bp$p.value, " and test statistic of ", bp$statistic,
          " are inconclusive, so homoschedasticity can't be determined using this test."))
  }

  # Adjusted R-squared
  adjrsq <- summary(lmod)$adj.r.squared
  print(paste0('Adjusted R-squared value: ', adjrsq))
    
  # Mean squared error
  m <- mse(lmod)
  print(paste0('Mean squared error: ', m))
  
  # Find leverage point cutoff
  n <- length(lmod$residuals)
  p <- length(summary(lmod)$coeff[,1] - 1)  # number of model parameters
  cutoff <- 2 * (p + 1) / n
  print(paste0('Leverage point cutoff: ', cutoff))
  
  # Variance inflation factor (VIF): 1=not correlated, 1-5=moderately correlated, >5=strongly correlated
  print('Variance inflation factor (VIF):')
  tryCatch(
      expr = {
        print(sort(vif(lmod), decreasing=T))
      },
      error = function(e) {
          print(paste0('error: ', e))
      }
  )
  
  # Standardized residual plots
  stanres <- rstandard(lmod)
  for (i in seq(1, ceiling(p / 4))) {
    par(mfrow=c(2, 2))
    starti <- ((i - 1) * 4) + 1
    for (j in seq(starti, starti + 3)) {
        if (j + 1 <= ncol(model.matrix(lmod))) {
        plot(model.matrix(lmod)[, j + 1], stanres)
      }
    }
  }

  # Find leverage points
  par(mfrow=c(2, 1))
  hv <- hatvalues(lmod)
  halfnorm(hv, nlab=3, labs=dfagg$unit, ylab='Leverage')
  cd <- cooks.distance(lmod)
  halfnorm(cd, nlab=3, labs=dfagg$unit, ylab="Cook's Distance")
  
  # Return value
  return(c(adjrsq, m, stret, bpret))
  
}

```

Now, perform an analysis on the best-performing models, and look for leverage points (outliers), i.e., points that have an strong influence over the resulting model.

```{r}

# Create a dataframe to display stats for all model runs
dfmodel_results <- data.frame(matrix(ncol = 7, nrow = 0))
colnames(dfmodel_results) <- c('Model_run', 'Model_formula', 'Insignificant_coefficients', 'Adjusted_R_squared', 'MSE', 'Breusch-Pagan', 'Shapiro')

# Model analysis
for (i in seq(1, length(lmod_form))) {
    title <- paste0(i, ': ', lmod_form[i])
    maret <- ModelAnalysis(lmod[[i]], title)
    dfmodel_results[i, 'Model_run'] <- i
    dfmodel_results[i, 'Model_formula'] <- title
    dfmodel_results[i, 'Adjusted_R_squared'] <- maret[1]
    dfmodel_results[i, 'MSE'] <- maret[2]
    dfmodel_results[i, 'Breusch-Pagan'] <- maret[4]
    dfmodel_results[i, 'Shapiro'] <- maret[3]
}

# Create list of adjusted R-squared values along with the number of insignificant coefficients (we won't want to use models that have insignificant coefficients)
adj_rsq <- c()
insignif_coef <- c()
for (i in seq(1, length(lmod))) {
    print(i)
    #adj_rsq <- c(adj_rsq, round(summary(lmod[[i]])$adj.r.squared, 3))
    tmp_coef <- summary(lmod[[i]])$coefficients
    print(paste0('here', tmp_coef))
    bad_coef <- 0
    for (j in seq(2, length(tmp_coef[,4]))) {
        print(paste0('\t',j))
        if (tmp_coef[j, 4] > 0.05) {
            bad_coef <- bad_coef + 1
        }
    }
    insignif_coef <- c(insignif_coef, bad_coef)
}
dfmodel_results$Insignificant_coefficients <- insignif_coef

# Display table
dfmodel_results

```


```{r fig.width=12}

# Blog
dfmodel_results_blog <- dfmodel_results

#knitr::kable(dfmodel_results, format = "html", table.attr = "style='width:100%;'")

library(kableExtra)
#kable(dfmodel_results) %>%
#  kable_styling(full_width = T)

dfmodel_results %>%
  kbl() %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>%
  kable_classic(full_width = F, font_size = 14)

```

As shown on the plots of residuals vs fitted values, there is visually noticeable heteroschedasticity, which invalidates the model. This is confirmed numerically by the Breusch-Pagan test, which generated high test statistics for both models as well as a low p-value. For a model to be considered homoschedastic, a low test statistic and p-values greater than or equal to 0.05 are required.


## Transformations

The modeling performed so far have left the variables untransformed. But because the untransformed models resulted in heteroschedastic residuals, a common approach in linear modeling too try correcting for this is to mathematically transform one or more of the variables in the model. We'll try several transformations on the response variable (endpoint count).

```{r}

# Generate new formulas with the response variable transformed (log, sqrt, and cubed root)
lmod_form_xform <- c()
for (i in seq(1, length(lmod_form))) {
    for (j in seq(1, 3)) {
        newform <- ''
        if (j == 1) {
            newform <- gsub(pattern='endpoint_ct', replacement='log(endpoint_ct)', x=lmod_form[i])
        }
        else if (j == 2) {
            newform <- gsub(pattern='endpoint_ct', replacement='sqrt(endpoint_ct)', x=lmod_form[i])
        }
        else if (j == 3) {
            newform <- gsub(pattern='endpoint_ct', replacement='(endpoint_ct)^(1/3)', x=lmod_form[i])
        }
        lmod_form_xform <- c(lmod_form_xform, newform)
    }
}

# Add transformed formulas to formula list
init_len <- length(lmod_form) + 1  # Save initial length so we know where to start modeling from (will be one more than this length)
lmod_form <- c(lmod_form, lmod_form_xform)

# Model using the new transforms
for (i in seq(init_len, length(lmod_form))) {
    lmod[[i]] <- lm(lmod_form[i], na.action=na.omit, data=dfagg)
    print("----------------------------------------------------------------------------------------------")
    print(paste0(i, ': ', lmod_form[i]))
    print(summary(lmod[[i]]))
}

```

Now perform model analysis on the transformed models.

```{r}

# Model analysis
for (i in seq(init_len, length(lmod_form))) {
    title <- paste0(i, ': ', lmod_form[i])
    maret <- ModelAnalysis(lmod[[i]], title)
    dfmodel_results[i, 'Model_run'] <- i
    dfmodel_results[i, 'Model_formula'] <- title
    dfmodel_results[i, 'Adjusted_R_squared'] <- maret[1]
    dfmodel_results[i, 'MSE'] <- maret[2]
    dfmodel_results[i, 'Shapiro'] <- maret[3]
    dfmodel_results[i, 'Breusch-Pagan'] <- maret[4]
}

```


## Model summary

Summary of model runs and results.

```{r}

# Create list of adjusted R-squared values along with the number of insignificant coefficients (we won't want to use models that have insignificant coefficients)
for (i in seq(init_len, length(lmod))) {
    adj_rsq <- c(adj_rsq, round(summary(lmod[[i]])$adj.r.squared, 3))
    tmp_coef <- summary(lmod[[i]])$coefficients
    bad_coef <- 0
    for (j in seq(2, length(tmp_coef[,4]))) {
        if (tmp_coef[j, 4] > 0.05) {
            bad_coef <- bad_coef + 1
        }
    }
    insignif_coef <- c(insignif_coef, bad_coef)
}
dfmodel_results$Insignificant_coefficients <- insignif_coef

# Display table
dfmodel_results

```


### Box-Cox transformations

Manually try some Box-Cox transformations on some of the better models to see if any improvement can be made.

```{r}

# Box-Cox transforms
bct1 <- powerTransform(cbind(endpoint_ct, mac_ct, fte_ct) ~ 1, dfagg)
print('Box-Cox transform on endpoints, mac, and fte: ')
print(bct1)
bct2 <- powerTransform(cbind(endpoint_ct, mac_ct) ~ 1, dfagg)
print('Box-Cox transform on endpoints and mac: ')
print(bct2)
bct3 <- powerTransform(cbind(endpoint_ct, fte_ct) ~ 1, dfagg)
print('Box-Cox transform on endpoints and fte: ')
print(bct3)
bct4 <- powerTransform(cbind(endpoint_ct) ~ 1, dfagg)
print('Box-Cox transform on endpoints only: ')
print(bct4)
bct5 <- powerTransform(cbind(mac_ct, fte_ct) ~ 1, dfagg)
print('Box-Cox transform on mac and fte only: ')
print(bct5)
bct6 <- powerTransform(cbind(mac_ct) ~ 1, dfagg)
print('Box-Cox transform on mac only: ')
print(bct6)
bct7 <- powerTransform(cbind(fte_ct) ~ 1, dfagg)
print('Box-Cox transform on fte only: ')
print(bct7)
dfagg <- dfagg %>%
    mutate(tendpoint_ct1 = endpoint_ct ^ bct1$lambda[1], tmac_ct1 = mac_ct ^ bct1$lambda[2], tfte_ct1 = fte_ct ^ bct1$lambda[3]) %>%
    mutate(tendpoint_ct2 = endpoint_ct ^ bct2$lambda[1], tmac_ct2 = mac_ct ^ bct2$lambda[2]) %>%
    mutate(tendpoint_ct3 = endpoint_ct ^ bct3$lambda[1], tfte_ct3 = fte_ct ^ bct3$lambda[2]) %>%
    mutate(tendpoint_ct4 = endpoint_ct ^ bct4$lambda[1]) %>%
    mutate(tmac_ct5 = mac_ct ^ bct5$lambda[1], tfte_ct5 = fte_ct ^ bct5$lambda[1]) %>%
    mutate(tmac_ct6 = mac_ct ^ bct6$lambda[1]) %>%
    mutate(tfte_ct7 = fte_ct ^ bct7$lambda[1])

# Add new formulas
init_len <- length(lmod_form) + 1
lmod_form <- c(lmod_form,
    'tendpoint_ct1 ~ tmac_ct1 + tfte_ct1',
    'tendpoint_ct2 ~ tmac_ct2',
    'tendpoint_ct3 ~ tfte_ct3',
    'tendpoint_ct4 ~ fte_ct + mac_ct',
    'tendpoint_ct4 ~ fte_ct + mac_ct + large_unit',
    'tendpoint_ct4 ~ fte_ct + mac_ct + doit_unit',
    'tendpoint_ct4 ~ fte_ct + mac_ct + large_unit + doit_unit',
    'tendpoint_ct4 ~ fte_ct + large_unit + doit_unit',
    'tendpoint_ct4 ~ fte_ct + large_unit:doit_unit',
    'tendpoint_ct4 ~ fte_ct + fte_ct:large_unit + fte_ct:doit_unit',
    'tendpoint_ct4 ~ mac_ct + large_unit',
    'endpoint_ct ~ tmac_ct5 + tfte_ct5',
    'endpoint_ct ~ tmac_ct6',
    'endpoint_ct ~ tfte_ct7'
)

# Model using the new transforms
for (i in seq(init_len, length(lmod_form))) {
    lmod[[i]] <- lm(lmod_form[i], na.action=na.omit, data=dfagg)
    print("----------------------------------------------------------------------------------------------")
    print(paste0(i, ': ', lmod_form[i]))
    print(summary(lmod[[i]]))
}

# Model analysis on the new transforms
for (i in seq(init_len, length(lmod_form))) {
    title <- paste0(i, ': ', lmod_form[i])
    maret <- ModelAnalysis(lmod[[i]], title)
    dfmodel_results[i, 'Model_run'] <- i
    dfmodel_results[i, 'Model_formula'] <- title
    dfmodel_results[i, 'Adjusted_R_squared'] <- maret[1]
    dfmodel_results[i, 'MSE'] <- maret[2]
    dfmodel_results[i, 'Shapiro'] <- maret[3]
    dfmodel_results[i, 'Breusch-Pagan'] <- maret[4]
}

# Create list of adjusted R-squared values along with the number of insignificant coefficients (we won't want to use models that have insignificant coefficients)
for (i in seq(init_len, length(lmod))) {
    adj_rsq <- c(adj_rsq, round(summary(lmod[[i]])$adj.r.squared, 3))
    tmp_coef <- summary(lmod[[i]])$coefficients
    bad_coef <- 0
    for (j in seq(2, length(tmp_coef[,4]))) {
        if (tmp_coef[j, 4] > 0.05) {
            bad_coef <- bad_coef + 1
        }
    }
    insignif_coef <- c(insignif_coef, bad_coef)
}
dfmodel_results$Insignificant_coefficients <- insignif_coef

# Display table
dfmodel_results

```


Display only those models with favourable results. I.e., only choose models with significant coefficients and those with homoschedastic, normally distributed residuals.

```{r}

# Display only models with favourable results
dfmodel_results %>%
    filter(Insignificant_coefficients == 0 & `Breusch-Pagan`=='homoschedastic' & Shapiro=='normal') %>%
    arrange(desc(Adjusted_R_squared))

# Blog
dfmodel_results %>%
    filter(Insignificant_coefficients == 0 & `Breusch-Pagan`=='homoschedastic' & Shapiro=='normal') %>%
    arrange(desc(Adjusted_R_squared)) %>%
  kbl() %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>%
  kable_classic(full_width = F, font_size = 14)
    


```


## Model selection

Now we have enough data to select the best-performing model that also conforms to the assumption of equal variance, normality of residuals, and independence of predictor variables. Based on the modeling results, the best model fits the cubed root of endpoint counts against the FTE counts with additional corrections for large units and DoIT. The adjusted R-squared value is high (0.881), which indicates a very strong relationship between endpoint count and the predictor variables. The mean squared error is also low (0.821), further confirming a good fit. Based on visual analysis of residuals and on evaluating the model using the Breusch-Pagan test, the residuals are homoschedastic, satisfying a critical component of model validation. An additional point of validation is that the residuals be close to normally distributed. Based on a visual observation and on the Shapiro test, this condition is also satisfied. In addition, the very small p-values indicate that all three predictors are statistically significant. As a final point of confirmation, variance inflation factor (VIF) values were examined to evaluate whether any colinearity exists between predictor pairs. The values ranged between 1.06 and 1.65, indicating very little colinearity.

In summary, there is a very strong, statistically significant relationship between the cubed root of endpoint count vs FTE count + large_unit + FTE_count:large_unit. The model appears to be valid both by visual observation and by numerical calculation using commonly accepted testing methods. The resulting model can be expressed as follows:

(endpoints) ^ 1/3 = 3.78638642 + (0.01604151 * fte_ct) + (2.96191105 * large_unit) + (6.21825879 * doit_unit)


## Confidence intervals

90% confidence intervals can be generated for the coefficients of the best model.

```{r}

# Calculate t value
best_i <- 48
lmod_best <- lmod[[best_i]]
lmod_best_sum <- summary(lmod_best)
lmod_best_sum$coefficients
tval <- qt(0.95, lmod_best$df.residual)

# Print best formula
print(lmod_form[best_i])

# Cycle through coefficients
for (i in seq(1, length(lmod_best_sum$coefficients[,1]))) {

    # Generate confidence interval
    tmp_name <- row.names(lmod_best_sum$coefficients)[i]  # coefficient name
    tmp_coef <- lmod_best_sum$coefficients[i, 1]  # coefficient value
    tmp_se <- lmod_best_sum$coefficients[i, 2]  # standard error
    print(paste0(tmp_name, ': ', tmp_coef, ' +/- ', tval * tmp_se, ' (', tmp_coef - (tval * tmp_se), ', ', tmp_coef + (tval * tmp_se), ')'))
    
}

```


(endpoints) ^ 1/3 = 3.78638642 + (0.01604151 * fte_ct) + (2.96191105 * large_unit) + (6.21825879 * doit_unit)


## Predict endpont counts

Now that we have a suitable model, we can predict endpoints based on the predictor variables that exhibited the best correlation (i.e., FTE counts, large_unit, and doit_unit).

```{r}

# Set prediction interval %
pred_int <- 90

# Group fte data by department, add in qualys and amp counts (zeroing out any NaNs), code for large unit
dfep <- dffte %>%
    group_by(div, dept) %>%
    summarize(fte_ct=sum(fte_count), .groups='keep') %>%
    rename(divcode=div, deptcode=dept) %>%
    full_join(dfamp_grp2, by=c('divcode', 'deptcode')) %>%
    full_join(dfqualys_grp2, by=c('divcode', 'deptcode')) %>%
    mutate(amp_ct=ifelse(is.na(amp_ct), 0, amp_ct), qualys_ct=ifelse(is.na(qualys_ct), 0, qualys_ct)) %>%
    mutate(doit_unit=ifelse(divcode == 'A06', TRUE, FALSE)) %>%
    mutate(large_unit=ifelse(deptcode %in% large_depts | divcode %in% large_divs, TRUE, FALSE)) %>%
    #mutate(large_unit=ifelse(divcode == 'A06', 2, ifelse(deptcode %in% large_depts | divcode %in% large_divs, 1, 0))) %>%
    mutate(fte_ct=ifelse(is.na(fte_ct), 0, fte_ct)) %>%
    arrange(divcode, deptcode)
dfep$doit_unit <- factor(dfep$doit_unit)
dfep$large_unit <- factor(dfep$large_unit)

# Generate prediction intervals
tmp_pred <- predict(lmod_best, newdata=dfep, interval="prediction", level=pred_int / 100)
tmp_pred <- data.frame(tmp_pred) %>%
    rename(predicted_total_endpoints=fit, predicted_total_endpoints_lo=lwr, predicted_total_endpoints_hi=upr)

# Adjust counts if the the endpoint count was transformed
if (substr(lmod_form[best_i], 1, str_length('(endpoint_ct)^(1/3)')) == '(endpoint_ct)^(1/3)') {
    tmp_pred <- tmp_pred ^ 3
} else if (substr(lmod_form[best_i], 1, str_length('sqrt(endpoint_ct)')) == 'sqrt(endpoint_ct)') {
    tmp_pred <- tmp_pred ^ 2
} else if (substr(lmod_form[best_i], 1, str_length('log(endpoint_ct)')) == 'log(endpoint_ct)') {
    tmp_pred <- exp(tmp_pred)
}

# Add the prediction columns onto the existing data frame
dfep <- dfep %>%
    cbind(tmp_pred)

# Assume the deployed endpoint count is the max of the qualys and amp counts
dfep$deployed_ct <- pmax(dfep$qualys_ct, dfep$amp_ct)

# Calculate the "adjusted" predicted total endpoints, which uses the actual endpoint counts for the "best" departments that we're assuming have full or near-full deployments
dfep <- dfep %>%
    mutate(adj_predicted_total_endpoints=ifelse(divcode %in% best_divs | deptcode %in% best_depts, deployed_ct, predicted_total_endpoints)) %>%
    mutate(adj_predicted_total_endpoints_lo=ifelse(divcode %in% best_divs | deptcode %in% best_depts, deployed_ct, predicted_total_endpoints_lo)) %>%
    mutate(adj_predicted_total_endpoints_hi=ifelse(divcode %in% best_divs | deptcode %in% best_depts, deployed_ct, predicted_total_endpoints_hi))

# Group by divcode
dfep2 <- dfep %>%
    group_by(divcode) %>%
    summarize(fte_ct=sum(fte_ct), predicted_total_endpoints=sum(predicted_total_endpoints), amp_ct=sum(amp_ct), qualys_ct=sum(qualys_ct), deployed_ct=sum(deployed_ct), 
        adj_predicted_total_endpoints=sum(adj_predicted_total_endpoints), adj_predicted_total_endpoints_lo=sum(adj_predicted_total_endpoints_lo),
        adj_predicted_total_endpoints_hi=sum(adj_predicted_total_endpoints_hi),.groups='keep') %>%
    filter(!is.na(fte_ct) & fte_ct > 0) %>%  # filter out na's, e.g. A90 - external entities who don't have any FTEs and which we don't care about tracking
    arrange(divcode)

# Change negative values to zero
dfep2 <- dfep2 %>%
    mutate(predicted_total_endpoints=ifelse(predicted_total_endpoints < 0, 0, predicted_total_endpoints)) %>%
    mutate(adj_predicted_total_endpoints=ifelse(adj_predicted_total_endpoints < 0, 0, adj_predicted_total_endpoints)) %>%
    mutate(adj_predicted_total_endpoints_hi=ifelse(adj_predicted_total_endpoints_hi < 0, 0, adj_predicted_total_endpoints_hi))

# Percent complete
dfep2 <- dfep2 %>%
    mutate(qualys_pct=qualys_ct / adj_predicted_total_endpoints) %>%
    mutate(amp_pct=amp_ct / adj_predicted_total_endpoints) %>%
    mutate(qualys_pct=ifelse(qualys_pct > 1.0, 1.0, qualys_pct)) %>%
    mutate(amp_pct=ifelse(amp_pct > 1.0, 1.0, amp_pct)) %>%
    mutate_all(~replace(., is.na(.), 0))

# Totals
total_deployed <- sum(dfep2$deployed_ct)
total_endpoints_lo <- sum(dfep2$adj_predicted_total_endpoints_lo)
total_endpoints <- sum(dfep2$adj_predicted_total_endpoints)
total_endpoints_hi <- sum(dfep2$adj_predicted_total_endpoints_hi)
print(paste0('AMP/Qualys deployed: ', total_deployed))
print(paste0('Total endpoints (', pred_int, '% lower bound): ', total_endpoints_lo))
print(paste0('Total endpoints: ', total_endpoints))
print(paste0('Total endpoints (', pred_int, '% upper bound): ', total_endpoints_hi))
print(paste0('Percent deployed: ', 100 * total_deployed / total_endpoints))

# Display results
dfep
dfep2

# Export to CSV
write.csv(dfep, '/Users/ippolito/Documents/c2e_endpoints_by_dept.csv')
write.csv(dfep2, '/Users/ippolito/Documents/c2e_endpoints_by_div.csv')

# Graph percentages - amp
dfep2 %>%
    ggplot(aes(reorder(divcode, desc(divcode)), amp_pct * 100)) +
    geom_bar(stat='identity') +
    coord_flip() +
    xlab('Division') +
    ylab('Cisco AMP % deployment')

# Graph percentages - qualys
dfep2 %>%
    ggplot(aes(reorder(divcode, desc(divcode)), qualys_pct * 100)) +
    geom_bar(stat='identity') +
    coord_flip() +
    xlab('Division') +
    ylab('Qualys % deployment')

dfep2_red <- dfep2
dfep2_red$rnd_num = sample(nrow(dfep2_red), nrow(dfep2_red), replace=F)
dfep2_red <- dfep2_red %>%
    mutate(divcode=gsub('A[0-9][0-9]', 'Axx', divcode)) %>% rename(unit=divcode) %>%
    arrange(rnd_num)
dfep2_red %>%
  kbl() %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>%
  kable_classic(full_width = F, font_size = 14)

```


## Conclusion

In conclusion, there is good statistical evidence to support a very strong linear model correlating endpoint count to FTE count, with additional categorical variables of large_unit and doit_unit. Commonly used techniques to evaluate the model's residuals indicate that the model is valid. However, the 90% prediction interval includes a wide range of values, resulting in a total predicted endpoint point estimate of 86,100, with lower and upper bounds of 43,600 and 169,000, respectively. Based on current deployment totals, we estimate the campus to have deployed Qualys and AMP to approximately 40,900 computers (47.5%).

